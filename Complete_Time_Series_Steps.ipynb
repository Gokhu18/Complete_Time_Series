{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Time series analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Data Extractor from yearly to monthly and daily data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function uses ffill methode for n-1 entries and bfill for n'th entry where n is the no of years present in data base \n",
    "\n",
    "Fixing up the gnalurity of data on required aggretration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "iport pandas as pd \n",
    "import datetime as dt\n",
    "\n",
    "def fun(i):\n",
    "    i=str(i)[4:]+\"-\"+str(int(i/100))\n",
    "    return i\n",
    "def monthly_to_Daily(df,column_name):\n",
    "    DF=df\n",
    "    DF[column_name]=DF[column_name].apply(fun)\n",
    "    DF[column_name]=DF[column_name].astype(str)\n",
    "    DF[column_name]='01-'+DF[column_name]\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name],format='%d-%m-%Y',errors='coerce')\n",
    "    DF=(DF.set_index(column_name).resample('D').pad().reset_index())\n",
    "    DF[column_name]=DF[column_name].astype(str)\n",
    "    DF[column_name][DF.shape[0]-1]=DF[column_name][DF.shape[0]-1][:7]+'-31'\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name],errors='coerce')\n",
    "    DF=DF.set_index(column_name).resample('D').bfill().reset_index()\n",
    "    return DF\n",
    "\n",
    "\n",
    "def Year_to_monthly(df,column_name):\n",
    "    DF=df\n",
    "    DF[column_name]=DF[column_name].astype(str)\n",
    "    DF[column_name]='01-01-'+DF[column_name]\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name])\n",
    "    DF=(DF.set_index(column_name).resample('M').pad().reset_index())\n",
    "    DF[column_name]=DF[column_name].astype(str)\n",
    "    DF[column_name][DF.shape[0]-1]=DF[column_name][DF.shape[0]-1][:4]+'-12-'+'31'\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name])\n",
    "    DF=DF.set_index(column_name).resample('M').bfill().reset_index()\n",
    "    return DF\n",
    "\n",
    "\n",
    "\n",
    "def Year_to_daily(df,column_name):\n",
    "    DF=df\n",
    "    DF[column_name]=DF[column_name].astype(str)\n",
    "    DF[column_name]='01-01-'+DF[column_name]\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name])\n",
    "    DF=(DF.set_index(column_name).resample('D').pad().reset_index())\n",
    "    DF[column_name]=DF[column_name].astype(str)\n",
    "    DF[column_name][DF.shape[0]-1]=DF[column_name][DF.shape[0]-1][:4]+'-12-'+'31'\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name])\n",
    "    DF=DF.set_index(column_name).resample('D').bfill().reset_index()\n",
    "    return DF\n",
    "\n",
    "\n",
    "def date_to_Yearly(df,column_name):\n",
    "    DF=df\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name],format='%d-%m-%Y')\n",
    "    DF[column_name]=DF[column_name].dt.year\n",
    "    DF=DF.groupby([column_name]).mean().reset_index()\n",
    "    return DF\n",
    "\n",
    "def date_to_Monthly(df,column_name):\n",
    "    DF=df\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name],format='%d-%m-%Y')\n",
    "    DF[column_name]=DF[column_name].map(lambda x: 100*x.year + x.month)\n",
    "    DF=DF.groupby([column_name]).mean().reset_index()\n",
    "    return DF\n",
    "\n",
    "def date_to_Quarterly(df,column_name):\n",
    "    DF=df\n",
    "    DF[column_name]=pd.to_datetime(DF[column_name],format='%d-%m-%Y')\n",
    "    DF[column_name]=DF[column_name].map(lambda x: 100*x.year + x.quarter)\n",
    "    DF=DF.groupby([column_name]).mean().reset_index()\n",
    "    return DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First and last missing index form base data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def missingIndex(DF_name,type_name):\n",
    "    DF14=DF_name\n",
    "    LS=[]\n",
    "    JS=[]\n",
    "    RS=[str(i) for i in DF14.columns]\n",
    "    for i in DF14.columns:\n",
    "        if type_name=='Last':\n",
    "            LS.append(DF14[str(i)].last_valid_index())\n",
    "        else:\n",
    "            LS.append(DF14[str(i)].First_valid_index())\n",
    "    for i in LS:\n",
    "        if i==None:\n",
    "            JS.append('None')\n",
    "        else:\n",
    "            JS.append(DF14['DATE'][int(i)])\n",
    "     \n",
    "    if type_name=='last':\n",
    "        final = pd.DataFrame(\n",
    "            {'last_Missing_Value_Index': JS,\n",
    "             'Coulumn_Name': RS\n",
    "             })\n",
    "        final.to_csv(\"last_Missing valueAPI4.csv\",sep=',',index=False)\n",
    "    else:\n",
    "        final = pd.DataFrame(\n",
    "            {'First_Missing_Value_Index': JS,\n",
    "             'Coulumn_Name': RS\n",
    "             })\n",
    "        final.to_csv(\"first_Missing valueAPI4.csv\",sep=',',index=False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Summary Generator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def summary_generator(fl_loc,file_names):\n",
    "\n",
    "### this complete code for univariate summary generation  is developed by Anurag Bhatt\n",
    "## provide the location of file(parent directory) in str format\n",
    "## file_name:  --- provide the list of files for which summary need to be generated..\n",
    "\n",
    "\n",
    "\n",
    "    import os\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import random\n",
    "\n",
    "    #######################################################################################################################\n",
    "    def is_datetime(col):\n",
    "        global valid_formats\n",
    "        global datetime_format\n",
    "        no_null = col.dropna()\n",
    "        no_null_list = list(no_null)\n",
    "        no_null_list = [x for x in no_null_list if x != 'nan']\n",
    "        no_null_list = [x for x in no_null_list if x != '0-Jan-00']\n",
    "    ##    sample_no_null = random.sample(list(no_null),100)\n",
    "        for iter_format in valid_formats:\n",
    "            try:\n",
    "                converted_sample = [datetime.strptime(str(x),iter_format) for x in no_null_list]\n",
    "                datetime_format = iter_format\n",
    "                tt = 1\n",
    "                break\n",
    "            except:\n",
    "                tt = 0\n",
    "        return tt\n",
    "\n",
    "    #######################################################################################################################\n",
    "    def is_numeric(col):\n",
    "        no_null = col.dropna()\n",
    "    ##    sample_no_null = random.sample(list(no_null),100)\n",
    "        try:\n",
    "            [float(str(x).replace(',','').replace('$','').replace('(','').replace(')','').replace(' ','')) for x in no_null]\n",
    "            tt = 1\n",
    "        except:\n",
    "            tt = 0\n",
    "        return tt\n",
    "\n",
    "    #######################################################################################################################\n",
    "    def get_dtype(file):\n",
    "        global categorical_cols\n",
    "        global numerical_cols\n",
    "        global datetime_cols\n",
    "        ncol = file.shape[1]\n",
    "        dtype = ['cat' for i in range(ncol)]\n",
    "\n",
    "        # auto-detecting the column type\n",
    "        for i in range(ncol):\n",
    "            col = file.iloc[:,i]\n",
    "            if is_numeric(col) == 1:\n",
    "                dtype[i] = 'nm'\n",
    "            elif is_datetime(col) == 1:\n",
    "                dtype[i] = 'dt'\n",
    "\n",
    "        # over writing the column type based on user input\n",
    "        for i in range(ncol):\n",
    "            if i in categorical_cols:\n",
    "                dtype[i] = 'cat'\n",
    "            elif i in numerical_cols:\n",
    "                dtype[i] = 'nm'\n",
    "            elif i in datetime_cols:\n",
    "                dtype[i] = 'dt'\n",
    "        return dtype\n",
    "\n",
    "    #######################################################################################################################\n",
    "    def get_num_info(col,i):\n",
    "        if is_numeric(col):\n",
    "            no_null = pd.Series([float(str(x).replace(',','').replace('$','').replace('(','').replace(')','').replace(' ','')) for x in col.dropna()])\n",
    "        ##    print(no_null)\n",
    "            try:\n",
    "                temp = ['numerical',col.name,i,len(col),col.isnull().sum(),(100*(col.isnull().sum())/(len(col))),len(no_null.unique()),no_null.mean()\n",
    "                        ,no_null.min(),no_null.quantile(q=0.01),no_null.quantile(q=0.1),no_null.quantile(q=0.05),no_null.quantile(q=0.25),\n",
    "                        no_null.quantile(q=0.5),no_null.quantile(q=0.75),no_null.quantile(q=0.9),no_null.quantile(q=0.95)\n",
    "                        ,no_null.quantile(q=0.99),no_null.max()]\n",
    "            except:\n",
    "                if (len(no_null.unique()) == 1):\n",
    "                    unique_el = float(no_null.unique()[0])\n",
    "                    temp = ['numerical',col.name,i,len(col),col.isnull().sum(),(100*(col.isnull().sum())/(len(col))),unique_el,unique_el, \\\n",
    "                                   unique_el,unique_el,unique_el,unique_el,unique_el,unique_el]\n",
    "                else:\n",
    "                    temp = ['numerical',col.name,i,'error','error','error','error','error','error','error','error','error','error','error']\n",
    "            print(temp)\n",
    "        else:\n",
    "            temp = ['numerical',col.name,i,'error','error','error','error','error','error','error','error','error','error','error']\n",
    "            raise ValueError('Column coercion did not work for this numerical column. Please check the coercion and try again. The column name and number are printed above.')\n",
    "        return temp\n",
    "\n",
    "    #######################################################################################################################\n",
    "    def get_cat_info(col,i):\n",
    "        # categorical coercion should always work !!\n",
    "        global n_freq\n",
    "        no_null = col.dropna()\n",
    "        try:\n",
    "            temp = ['categorical',col.name,i,len(col),col.isnull().sum(),(100*(col.isnull().sum())/(len(col))),len(no_null.unique()),len(no_null) - no_null.value_counts().iloc[:n_freq].sum()]\n",
    "            temp2 = [[x,y] for (x,y) in zip(no_null.value_counts().index[:n_freq].tolist(),no_null.value_counts().iloc[:n_freq].tolist())]\n",
    "            for i in temp2:\n",
    "                temp.extend(i)\n",
    "        except:\n",
    "            temp = ['categorical',col.name,i,'error','error','error','error','error','error','error','error','error','error','error']\n",
    "        print(temp)\n",
    "        return temp\n",
    "    #######################################################################################################################\n",
    "    def get_date_info(col,i):\n",
    "        global datetime_format\n",
    "        if is_datetime(col):\n",
    "            no_null = col.dropna()\n",
    "            no_null_list = list(no_null)\n",
    "            no_null_list = [x for x in no_null_list if x != 'nan']\n",
    "            no_null_list = [x for x in no_null_list if x != '0-Jan-00']\n",
    "            no_null_list = [datetime.strptime(str(x),datetime_format) for x in no_null_list]\n",
    "            try:\n",
    "                temp = ['date',col.name,i,len(col),len(col)-len(no_null_list),(100*(len(col)-len(no_null_list))/(len(col))),min(no_null_list),max(no_null_list)]\n",
    "            except:\n",
    "                temp = ['date',col.name,i,'error','error','error','error','error']\n",
    "        else:\n",
    "            temp = ['date',col.name,i,'error','error','error','error','error']\n",
    "            raise ValueError('Column coercion did not work for this datetime column. Please check the coercion and try again. The column name and number are printed above.')\n",
    "        print(temp)\n",
    "        return temp\n",
    "\n",
    "    #######################################################################################################################\n",
    "    #######################################################################################################################\n",
    "    #######################################################################################################################\n",
    "\n",
    "    os.chdir(fl_loc,)\n",
    "\n",
    "\n",
    "    #sys.stdout=open(\"external.log\",\"w\")\n",
    "\n",
    "    # these are user inputs\n",
    "    global n_freq\n",
    "    global valid_formats\n",
    "    global categorical_cols\n",
    "    global numerical_cols\n",
    "    global datetime_cols\n",
    "    global datetime_format\n",
    "    n_freq = 25\n",
    "    valid_formats = [\"%Y-%m-%d\",\"%d.%m.%y\",\"%d.%M.%Y\",\"%d-%m-%Y %I:%M\",\"%d-%m-%y %I:%M\",\"%d-%m-%Y %I:%M:%S %p\",\n",
    "                     \"%Y-%m-%d %I:%M:%S %p\",\"%d-%m-%y\",\"%Y-%m-%d %I:%M:%S\",\"%Y-%m-%d %H:%M:%S\",\"%Y%m\",\"%m/%d/%Y %H:%M\",\n",
    "                     \"%d-%m-%Y\",\"%m/%d/%Y\",\"%d-%m-%Y %I:%M:%S %p\",\"%d-%m-%Y %H:%M\",\"%d-%m-%y %H:%M\",\"%d-%b-%y\",\"%b-%y\"\n",
    "                     ]\n",
    "\n",
    "    categorical_cols = []\n",
    "    numerical_cols = []\n",
    "    datetime_cols = []\n",
    "\n",
    "    file_list=file_names\n",
    "\n",
    "    ##file_list = os.listdir()\n",
    "    writer = pd.ExcelWriter('summary_API.xlsx')\n",
    "    for file_name in file_list:\n",
    "        print(file_name)\n",
    "        # checking the file type using the extension and reading accordingly\n",
    "        if ((file_name.split('.')[-1].lower() == 'xlsx') or (file_name.split('.')[-1].lower() == 'xls')):\n",
    "            file = pd.read_excel(file_name)\n",
    "        elif (file_name.split('.')[-1].lower() == 'csv'):\n",
    "            file = pd.read_csv(file_name)\n",
    "        print(file.shape)\n",
    "        dtype = get_dtype(file)\n",
    "        temp_cat = []\n",
    "        temp_num = []\n",
    "        temp_date = []\n",
    "        for i in range(len(dtype)):\n",
    "        ##    print(dtype[i])\n",
    "            tt = file.iloc[:,i]\n",
    "            print('Executing for column number : ',i,' Name of the column : '+tt.name)\n",
    "            if dtype[i] == 'cat':\n",
    "                temp_cat.append(get_cat_info(tt,i))\n",
    "            elif dtype[i] == 'nm':\n",
    "                temp_num.append(get_num_info(tt,i))\n",
    "            elif dtype[i] == 'dt':\n",
    "                temp_date.append(get_date_info(tt,i))\n",
    "\n",
    "        if len(temp_num)>0:\n",
    "            temp_num = pd.DataFrame(temp_num)\n",
    "            temp_num.columns = ['type','col_name','col_number','count','missing','missing perc','number of unique','mean',\n",
    "                                'minimum','1st percentile','5th percentile','10th percentile','25th percentile','50th percentile',\n",
    "                                '75th percentile','90th percentile','95th percentile','99th percentile','maximum']\n",
    "    ##        temp_num.to_csv(file_name.split('.')[0]+'_num.csv',index=False)\n",
    "            temp_num.to_excel(writer,sheet_name = file_name.split('.')[0]+'_num')\n",
    "\n",
    "        if len(temp_date)>0:\n",
    "            temp_date = pd.DataFrame(temp_date)\n",
    "            temp_date.columns = ['type','col_name','col_number','count','missing','missing perc','range from','range to']\n",
    "    ##        temp_date.to_csv(file_name.split('.')[0]+'_date.csv',index=False)\n",
    "            temp_date.to_excel(writer,sheet_name = file_name.split('.')[0]+'_date')\n",
    "\n",
    "        if len(temp_cat)>0:\n",
    "            temp_cat = pd.DataFrame(temp_cat)\n",
    "            tt2 = ['type','col_name','col_number','count','missing','missing perc','number of unique values','count of others']\n",
    "            col_name_temp=[]\n",
    "            col_temp = ['value','freq']\n",
    "            n_tt = int((temp_cat.shape[1]-7)/2)\n",
    "            for i in range(n_tt):\n",
    "                col_name_temp.extend(col_temp)\n",
    "            tt2.extend(col_name_temp)\n",
    "            temp_cat.columns = tt2\n",
    "    ##        temp_cat.to_csv(file_name.split('.')[0]+'_cat.csv',index=False)\n",
    "            temp_cat.to_excel(writer,sheet_name = file_name.split('.')[0]+'_cat')\n",
    "\n",
    "    writer.save()\n",
    "    #sys.stdout.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise analysis of data "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### path = provide the path of the folder which you want to make the base folder for analysis\n",
    "###File_Name= Provide the file name on which base data is saved\n",
    "### Date_Column= provide the date column name in the base file\n",
    "### Usable_Per= provide the max percentage of data you want keep ex. if u give 80 here the column having more \n",
    "### then 80% missing values will be dropped \n",
    "\n",
    "\n",
    "def sdad(Path,File_Name,Date_column,Usable_Per):\n",
    "    os.chdir(Path)\n",
    "    df=pd.read_csv(File_Name)\n",
    "    df[Date_column]=pd.to_datetime(df[Date_column],format='%d-%m-%Y')\n",
    "    a=[]\n",
    "    for i in  df.columns:\n",
    "        percent_missing = df[str(i)].isnull().sum() * 100 / len(df)\n",
    "        if percent_missing > Usable_Per:\n",
    "            df=df.drop([str(i)],axis=1)\n",
    "            a.append(str(i))\n",
    "    print(\"No of Columns dropped:-\",len(a))\n",
    "    print(\"Columns dropped:-\",a)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit root dicifuller and KPSS testing and diffrencing of data for stationarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## dici function gives us augumented dicifuller results and stationarised variable in the file we need to give the data_frame in the\n",
    "## input and if returens two data frame one which stationarised time series data and other with order of diff required to make \n",
    "## time series stationary\n",
    "## it first takes log transform for all data \n",
    "\n",
    "\n",
    "def differencing(y,n):\n",
    "    if n > 0:\n",
    "        tt = []\n",
    "        for i in range(len(y)-n):\n",
    "            tt.append(y[i+n]-y[i])\n",
    "        return tt\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "### Function for dicifuller and kpss test \n",
    "def dici(df):\n",
    "    d={}\n",
    "    o={}\n",
    "    for i in df.columns:\n",
    "        try:\n",
    "            jamu=i\n",
    "            if type(df[str(i)][0])==numpy.float64:\n",
    "                order=0\n",
    "                a=df[i].first_valid_index()\n",
    "                b=df[i].last_valid_index()\n",
    "                Target_Sub = df[['DATE',str(i)]].loc[range(a,b)]\n",
    "                Target_Sub[str(i)]=Target_Sub[str(i)].fillna(method='pad')\n",
    "                dftest=st.adfuller(Target_Sub[str(i)].loc[range(a,b)].values, autolag='AIC')\n",
    "                kpsstest=st.kpss(Target_Sub[str(i)].loc[range(a,b)].values)\n",
    "                if dftest[1]>.05 or kpsstest[1]<.05:\n",
    "                    mute=pd.Series(differencing(Target_Sub[str(i)].loc[range(a,b)].values,1))\n",
    "                    Target_Sub = Target_Sub.reset_index(drop=True)\n",
    "                    Target_Sub[str(i)]=mute\n",
    "                    a=Target_Sub[str(i)].first_valid_index()\n",
    "                    b=Target_Sub[str(i)].last_valid_index()\n",
    "                    dftest=st.adfuller(Target_Sub[str(i)].loc[range(a,b)].values,autolag='AIC')\n",
    "                    order=1              \n",
    "                    if dftest[1]>.05:\n",
    "                        a=Target_Sub[str(i)].first_valid_index()\n",
    "                        b=Target_Sub[str(i)].last_valid_index()\n",
    "                        mute=pd.Series(differencing(Target_Sub[str(i)].loc[range(a,b)].values,1))\n",
    "                        Target_Sub = Target_Sub.reset_index(drop=True)\n",
    "                        Target_Sub[str(i)]=mute\n",
    "                        order=2                  \n",
    "                    d[str(i)]=Target_Sub\n",
    "                    o[str(i)]=order\n",
    "\n",
    "                else:  \n",
    "                    d[str(i)]=df[str(i)]\n",
    "                    o[str(i)]=order\n",
    "        except:\n",
    "            pass\n",
    "    new=d[list(d.keys())[0]]\n",
    "    for i in range(1,len(d.keys())):\n",
    "        try:\n",
    "            new=new.merge(d[list(d.keys())[i]],on='DATE',how='left')\n",
    "        except:\n",
    "            pass\n",
    "    return new\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .....using Granger casulity test on variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# column_list: is the list of columns \n",
    "#data_frame: complete dataframe you have give in input\n",
    "##provide string of 'Lead' or 'lag'\n",
    "# Max lag:- will have no of lags required for you to analysis\n",
    "#Master_Var:- Dependent variable name\n",
    "\n",
    "def grangertest(Master_Var,column_list,data_frame,lead_or_lag,maxlags):\n",
    "    STD=data_frame\n",
    "    STD_col=column_list\n",
    "    dd={}\n",
    "    if lead_or_lag == 'Lead':\n",
    "        for i in STD_col:\n",
    "            try:\n",
    "                if type(STD[str(i)][0])==numpy.float64:             \n",
    "                    if STD[Master_Var].first_valid_index()>STD[str(i)].first_valid_index():\n",
    "                        a=STD[Master_Var].first_valid_index()\n",
    "                    else:\n",
    "                        a=STD[str(i)].first_valid_index()\n",
    "                    if STD[Master_Var].last_valid_index()<STD[str(i)].last_valid_index():\n",
    "                        b=STD[Master_Var].last_valid_index()\n",
    "                    else:\n",
    "                        b=STD[str(i)].last_valid_index()                                \n",
    "                    Target_Sub = STD[[Master_Var,str(i)]].loc[range(a,b)]\n",
    "                    Target_Sub=Target_Sub.interpolate()\n",
    "                    Target_Sub=Target_Sub.fillna(0)\n",
    "\n",
    "                    m=st.grangercausalitytests(Target_Sub, maxlag=maxlags, addconst=True, verbose=True)\n",
    "                    a=[]\n",
    "                    for x in range(1,maxlags):\n",
    "                        a.append(m[x][0]['ssr_chi2test'][1])\n",
    "                dd[str(i)]=a\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    elif lead_or_lag == 'Lag':\n",
    "        for i in STD_col:\n",
    "            try:\n",
    "                if type(STD[str(i)][0])==numpy.float64: \n",
    "                    if STD[Master_Var].first_valid_index()>STD[str(i)].first_valid_index():\n",
    "                        a=STD[Master_Var].first_valid_index()\n",
    "                    else:\n",
    "                        a=STD[str(i)].first_valid_index()\n",
    "                    if STD[Master_Var].last_valid_index()<STD[str(i)].last_valid_index():\n",
    "                        b=STD[Master_Var].last_valid_index()\n",
    "                    else:\n",
    "                        b=STD[str(i)].last_valid_index()                                \n",
    "                    Target_Sub = STD[[Master_Var,str(i)]].loc[range(a,b)]\n",
    "                    Target_Sub=Target_Sub.interpolate()\n",
    "                    Target_Sub=Target_Sub.fillna(0)\n",
    "                    m=st.grangercausalitytests(Target_Sub, maxlag=maxlags, addconst=True, verbose=True)\n",
    "                    a=[]\n",
    "                    for x in range(1,maxlags):\n",
    "                        a.append(m[x][0]['ssr_chi2test'][1])\n",
    "                dd[str(i)]=a\n",
    "            except:\n",
    "                pass\n",
    "    casu = pd.DataFrame({ key:pd.Series(value) for key, value in dd.items() })\n",
    "#     try:\n",
    "#         a=pd.DataFrame(casu.idxmin())+1\n",
    "#         b=pd.DataFrame(casu.min())\n",
    "#         b.columns=['MIn_P_Value']\n",
    "#         a.columns=['Best_lag']\n",
    "#     except:\n",
    "#         a=1000\n",
    "#         b=1000\n",
    "#         pass\n",
    "\n",
    "    return(casu.T)\n",
    "    \n",
    "    \n",
    "    ### the lag column is starting from 0 to maxlag you need to change it to 1 to max lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCF plot and csv file generator with diffrenced and undiffrenced data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "\n",
    "# Provide the base data here \n",
    "\n",
    "HBAdat1 = pandas.read_excel(, header=0)\n",
    "\n",
    "\n",
    "for col in HBAdat1:\n",
    "    col_new = col.replace('/','_').replace('.','_').replace('-','_').replace('(','').replace(')','').replace('%','_').replace('&','_').replace(' ','').replace('\"','').replace('?','__')\n",
    "    HBAdat1.rename(columns={col:col_new}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def differencing(y,n):\n",
    "    if n > 0:\n",
    "        tt = []\n",
    "        for i in range(len(y)-n):\n",
    "            tt.append(y[i+n]-y[i])\n",
    "        return tt\n",
    "    else:\n",
    "        return y\n",
    "    \n",
    "def plot_ACF(y_avg,n):\n",
    "    n_lags = len(y_avg)//2\n",
    "    y_avg = differencing(y_avg,n)\n",
    "    results_acf = acf(y_avg, nlags=n_lags-1)\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(range(n_lags),results_acf,'-*')\n",
    "    plt.minorticks_on()\n",
    "    plt.axhline(y=0,c='green',ls='--')\n",
    "    plt.axhline(y=0.2,c='orange',ls='--')\n",
    "    plt.axhline(y=-0.2,c='orange',ls='--')\n",
    "    plt.grid(True,which='both')\n",
    "    plt.ylabel(\"value of ACF\")\n",
    "    plt.title(\"ACF Plot\")\n",
    "\n",
    "    results_pacf = pacf(y_avg, nlags=n_lags-1,method = 'ols')\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(range(n_lags),results_pacf,'-*')\n",
    "    plt.minorticks_on()\n",
    "    plt.xlabel(\"lag\")\n",
    "    plt.axhline(y=0,c='green',ls='--')\n",
    "    plt.axhline(y=0.2,c='orange',ls='--')\n",
    "    plt.axhline(y=-0.2,c='orange',ls='--')\n",
    "    plt.grid(True,which='both')\n",
    "    plt.ylabel(\"value of PACF\")\n",
    "    plt.title(\"PACF Plot\")\n",
    "\n",
    "    plt.suptitle('ACF and PACF plots with differencing = {0}'.format(n))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "var=[\"API2\",\"API4\",\"HBA\",\"NEW_C\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for tgt in var:\n",
    "    os.chdir(\"D:/bloomberg_Data/NEW/\"+tgt+\"/\")\n",
    "    correlation_details = []\n",
    "    varcnt = 1\n",
    "    for col in HBAdat1:\n",
    "#         print(col + \":\" + str(varcnt))\n",
    "        varcnt = varcnt + 1\n",
    "        if col in HBAdat1.columns:\n",
    "            try:\n",
    "                plt.clf()\n",
    "                fig = plt.figure(figsize=(20,10))\n",
    "                ax1 = plt.subplot2grid((3, 2), (0, 0), colspan=2, rowspan=2)\n",
    "                ax1.plot(HBAdat1['DATE'], HBAdat1['HBA'], 'b-')\n",
    "                ax1.set_ylabel('HBA', color='b')\n",
    "\n",
    "                ax2 = ax1.twinx()\n",
    "                ax2.plot(HBAdat1['DATE'], HBAdat1[col], 'g-')\n",
    "                ax2.set_ylabel(col, color='g')\n",
    "                plt.title('Variables Plot')      \n",
    "                datC = []\n",
    "                for i in range(-24,25):\n",
    "                    datZ = HBAdat1.loc[:,[tgt,col]].copy()\n",
    "                    datZ[col] = datZ[col].shift(-i)\n",
    "                    datZ = datZ[np.isfinite(datZ[tgt]) & np.isfinite(datZ[col])]\n",
    "                    datC.append(datZ)\n",
    "\n",
    "                corr1 = []\n",
    "                for i in range(49):\n",
    "                    if len(datC[i].index) > 40:\n",
    "                        corr1.append(np.corrcoef(datC[i][tgt],datC[i][col])[0][1])\n",
    "                    else:\n",
    "                        corr1.append(0)\n",
    "\n",
    "                plt.subplot2grid((3, 2), (2, 0), colspan=1, rowspan=1)\n",
    "                plt.bar(list(range(-24,25)),corr1)\n",
    "                plt.title('Lead & Lag Correlation Plots - Original Vars')\n",
    "\n",
    "                datC = []\n",
    "                for i in range(-24,25):\n",
    "                    datZ = HBAdat1.loc[:,[tgt,col]].copy()\n",
    "                    datZ[col] = datZ[col].shift(-i)\n",
    "                    datZ = datZ[np.isfinite(datZ[tgt]) & np.isfinite(datZ[col])]\n",
    "                    datC.append(datZ)\n",
    "\n",
    "                corr2 = []\n",
    "                for i in range(49):\n",
    "                    if len(datC[i].index) > 40:\n",
    "                        var1 = differencing(datC[i][tgt].values,1)\n",
    "                        var2 = differencing(datC[i][col].values,1)\n",
    "                        corr2.append(np.corrcoef(var1,var2)[0][1])\n",
    "                    else:\n",
    "                        corr2.append(0)\n",
    "\n",
    "                plt.subplot2grid((3, 2), (2, 1), colspan=1, rowspan=1)\n",
    "                plt.bar(list(range(-24,25)),corr2)\n",
    "                plt.title('Lead & Lag Correlation Plots - Differenced Vars')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(col)\n",
    "                plt.close()\n",
    "\n",
    "                correlation_details.append([col,max(max(corr1),min(corr1),key=abs),corr1.index(max(max(corr1),min(corr1),key=abs))-24,max(max(corr2),min(corr2),key=abs),corr2.index(max(max(corr2),min(corr2),key=abs))-24])\n",
    "            except:\n",
    "                pass\n",
    "    correlation_df = pandas.DataFrame(correlation_details,columns=['Column','Orig Max Corr','Orig Max Corr Lag','Differenced Max Corr','Differenced Max Corr Index'])\n",
    "    correlation_df.to_csv(\"CorrelationDataFrame_\"+tgt+\"v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only CCF History CSV file generator it will not generate any graphs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "\n",
    "\n",
    "\n",
    "# Provide the base data here \n",
    "\n",
    "HBAdat1 = df1\n",
    "\n",
    "\n",
    "for col in HBAdat1:\n",
    "    col_new = col.replace('/','_').replace('.','_').replace('-','_').replace('(','').replace(')','').replace('%','_').replace('&','_').replace(' ','').replace('\"','').replace('?','__')\n",
    "    HBAdat1.rename(columns={col:col_new}, inplace=True)\n",
    "\n",
    "def differencing(y,n):\n",
    "    if n > 0:\n",
    "        tt = []\n",
    "        for i in range(len(y)-n):\n",
    "            tt.append(y[i+n]-y[i])\n",
    "        return tt\n",
    "    else:\n",
    "        return y\n",
    "    \n",
    "var=df1.columns[15:]\n",
    "for tgt in var:\n",
    "    os.chdir('D:/Investing.com_data')\n",
    "    try:\n",
    "        os.mkdir(tgt)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(\"D:/Investing.com_data/\"+ tgt + \"/\" )\n",
    "    correlation_details = []\n",
    "    varcnt = 1\n",
    "    for col in HBAdat1:\n",
    "        print(col + \":\" + str(varcnt))\n",
    "        varcnt = varcnt + 1\n",
    "        if col in HBAdat1.columns:\n",
    "            try:\n",
    "                if col=='DATE':\n",
    "                    pass\n",
    "                else:\n",
    "                    datC = []\n",
    "                    for i in range(-5,5):\n",
    "                        datZ = HBAdat1.loc[:,[tgt,col]].copy()\n",
    "                        datZ[col] = datZ[col].shift(-i)\n",
    "                        datZ = datZ[np.isfinite(datZ[tgt]) & np.isfinite(datZ[col])]\n",
    "                        datC.append(datZ)\n",
    "\n",
    "                    corr1 = []\n",
    "                    for i in range(10):\n",
    "                        if len(datC[i].index) > 10:\n",
    "                            corr1.append(np.corrcoef(datC[i][tgt],datC[i][col])[0][1])\n",
    "                        else:\n",
    "                            corr1.append(0)\n",
    "                    datC = []\n",
    "                    for i in range(-5,5):\n",
    "                        datZ = HBAdat1.loc[:,[tgt,col]].copy()\n",
    "                        datZ[col] = datZ[col].shift(-i)\n",
    "                        datZ = datZ[np.isfinite(datZ[tgt]) & np.isfinite(datZ[col])]\n",
    "                        datC.append(datZ)\n",
    "\n",
    "                    corr2 = []\n",
    "                    for i in range(10):\n",
    "                        if len(datC[i].index) > 10:\n",
    "                            var1 = differencing(datC[i][tgt].values,1)\n",
    "                            var2 = differencing(datC[i][col].values,1)\n",
    "                            corr2.append(np.corrcoef(var1,var2)[0][1])\n",
    "                        else:\n",
    "                            corr2.append(0)\n",
    "                    correlation_details.append([col,max(max(corr1),min(corr1),key=abs),corr1.index(max(max(corr1),min(corr1),key=abs))-5,max(max(corr2),min(corr2),key=abs),corr2.index(max(max(corr2),min(corr2),key=abs))-5])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "                  \n",
    "    correlation_df = pandas.DataFrame(correlation_details,columns=['Column','Orig Max Corr','Orig Max Corr Lag','Differenced Max Corr','Differenced Max Corr Index'])\n",
    "    correlation_df.to_csv(\"CorrelationDataFrame_\"+tgt+\"v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality and trends testing :- If seasonality present then decompose it "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(data, model='multiplicative') \n",
    "result.plot()\n",
    "plt.savefig('seasonal_decompose.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend testing MK code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mk_test(x, alpha=0.05):\n",
    "    \n",
    "    n = len(x)\n",
    "\n",
    "    # calculate S\n",
    "    s = 0\n",
    "    for k in range(n-1):\n",
    "        for j in range(k+1, n):\n",
    "            s += np.sign(x[j] - x[k])\n",
    "\n",
    "    # calculate the unique data\n",
    "    unique_x = np.unique(x)\n",
    "    g = len(unique_x)\n",
    "\n",
    "    # calculate the var(s)\n",
    "    if n == g:  # there is no tie\n",
    "        var_s = (n*(n-1)*(2*n+5))/18\n",
    "    else:  # there are some ties in data\n",
    "        tp = np.zeros(unique_x.shape)\n",
    "        for i in range(len(unique_x)):\n",
    "            tp[i] = sum(x == unique_x[i])\n",
    "        var_s = (n*(n-1)*(2*n+5) - np.sum(tp*(tp-1)*(2*tp+5)))/18\n",
    "\n",
    "    if s > 0:\n",
    "        z = (s - 1)/np.sqrt(var_s)\n",
    "    elif s == 0:\n",
    "            z = 0\n",
    "    elif s < 0:\n",
    "        z = (s + 1)/np.sqrt(var_s)\n",
    "\n",
    "    # calculate the p_value\n",
    "    p = 2*(1-norm.cdf(abs(z)))  # two tail test\n",
    "    h = abs(z) > norm.ppf(1-alpha/2)\n",
    "\n",
    "    if (z < 0) and h:\n",
    "        trend = 'decreasing'\n",
    "    elif (z > 0) and h:\n",
    "        trend = 'increasing'\n",
    "    else:\n",
    "        trend = 'no trend'\n",
    "\n",
    "    return trend, h, p, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very Basic AREMA MODEl:-- for checking the order of AR and MA component generate ACF and PACF plot "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## PACF: will tell the AR component \n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf((STD[index].fillna(method='pad')),lags=20)\n",
    "plt.savefig('Partial_Autocorrelation.jpg')\n",
    "\n",
    "## ACF : Will tell the MA component \n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf((STD[index].fillna(method='pad')),lags=20)\n",
    "plt.savefig('Autocorrelation.jpg')\n",
    "\n",
    "\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "series = STD.API2.values\n",
    "model = ARIMA(series, order=(2,0,0))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "residuals = pd.DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.title('plot of residuals')\n",
    "pyplot.show()\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoAREMA range of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use autoarema model it uses AIC and BIC and run the AREMA model in a loop(all value of AR and MA component) and pick the least value AREMA model for checks\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyramid.arima import auto_arima\n",
    "stepwise_model = auto_arima(data,start_p=0, start_q=0,\n",
    "                           max_p=3, max_q=3, m=12,\n",
    "                           start_P=0, seasonal=True,\n",
    "                           d=1, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)\n",
    "print(stepwise_model.aic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dignosis\n",
    "\n",
    "Generate the summary of the models. It will provide you the coeff AIC and BIC values \n",
    "Skew and hateroskedasticity values "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stepwise_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normality of the residuals\n",
    "\n",
    "You can generate the Q-Q plots of residuals for testing if it follows normal distribution.\n",
    "Also need to plot the residuals if there is some pattern present"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Q-Q plots of residuals \n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pylab\n",
    "\n",
    "test = np.random.normal(0,1, 1000)\n",
    "\n",
    "sm.qqplot(test, line='45')\n",
    "pylab.show()\n",
    "\n",
    "\n",
    "# Jarque-Bera test:\n",
    "\n",
    "\n",
    "name = ['Jarque-Bera', 'Chi^2 two-tail prob.', 'Skew', 'Kurtosis']\n",
    "test = sms.jarque_bera(results.resid)\n",
    "lzip(name, test)\n",
    "\n",
    "# Omni test\n",
    "\n",
    "name = ['Chi^2', 'Two-tail probability']\n",
    "test = sms.omni_normtest(results.resid)\n",
    "lzip(name, test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Influence tests\n",
    "Once created, an object of class OLSInfluence holds attributes and methods that allow users to assess the influence of each observation. For example, we can compute and extract the first few rows of DFbetas by:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "test_class = OLSInfluence(results)\n",
    "test_class.dfbetas[:5,:]\n",
    "\n",
    "\n",
    "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig = plot_leverage_resid2(results, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicollinearity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.linalg.cond(results.model.exog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroskedasticity tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some test for Heteroscedasticity\n",
    "\n",
    "Tests in regression\n",
    "1.Levene's test  = null hypothesis(variances of the populations from which different samples are drawn are equal)\n",
    "                    Critical p- values(0.05)\n",
    "                    \n",
    "         code       [scipy.stats.levene(*args, **kwds)]\n",
    "                    \n",
    "2.Goldfeld–Quandt test\n",
    "\n",
    "\n",
    "3.Park test[14]\n",
    "4.Glejser test[15][16]\n",
    "5.Brown–Forsythe test\n",
    "6.Harrison–McCabe test\n",
    "7.Breusch–Pagan test\n",
    "8.White test[5]\n",
    "9.Cook–Weisberg test\n",
    "10.Tests for grouped data\n",
    "11.F-test of equality of variances\n",
    "12.Cochran's C test\n",
    "13.Hartley's test\n",
    "\n",
    "https://en.wikipedia.org/wiki/Heteroscedasticity\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Breush-Pagan test:\n",
    "\n",
    "name = ['Lagrange multiplier statistic', 'p-value',\n",
    "        'f-value', 'f p-value']\n",
    "test = sms.het_breuschpagan(results.resid, results.model.exog)\n",
    "lzip(name, test)\n",
    "\n",
    "# Goldfeld-Quandt test\n",
    "\n",
    "name = ['F statistic', 'p-value']\n",
    "test = sms.het_goldfeldquandt(results.resid, results.model.exog)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearity\n",
    "Harvey-Collier multiplier test for Null hypothesis that the linear specification is correct:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name = ['t value', 'p value']\n",
    "test = sms.linear_harvey_collier(results)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the forecast of form the models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "future_forecast=stepwise_model.predict(n_periods=6)\n",
    "\n",
    "future_forecast = pd.DataFrame(future_forecast,index = test.index,columns=['Prediction'])\n",
    "actual_vs_prediction=pd.concat([(test),(future_forecast)],axis=1)\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot_date(future_forecast.index,actual_vs_prediction,linestyle='-')\n",
    "plt.legend(['Actual', 'Predicted'],fontsize=20)\n",
    "plt.title('AREMA on HBA Monthly',fontsize=40)\n",
    "plt.tick_params('x',labelsize=20)\n",
    "plt.tick_params('y',labelsize=20)\n",
    "# plt.set_xlabel(Date_Column,fontsize=18)\n",
    "plt.savefig('Arema_API2_Monthly.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMAX Range of models including exog variables\n",
    "\n",
    "these model can incoporate the exog variable inside and are vary accurate with CCF data \n",
    "\n",
    "based on CCF matrix we train the model on shifted data and generate the forcast "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pyflux as pf\n",
    "model = pf.ARIMAX(data=data, formula='drivers~1+seat_belt+oil_crisis',\n",
    "                  ar=1, ma=1, family=pf.Normal())\n",
    "x = model.fit(\"MLE\")\n",
    "x.summary()\n",
    "\n",
    "ARIMAX(1,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAREMA : if seasonality is present in the data you can use SAREMAX model and provide the order of seasonality(after how many observation there is repeatation of trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### (S)ARIMA(X) RANGE OF MODELS ##########\n",
    "o=[]\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "RFinal_Data=np.log(STD.bfill().ffill())\n",
    "for i in range(12,1,-1):\n",
    "    Final_Data=RFinal_Data[:RFinal_Data.index[-i]]\n",
    "    new=RFinal_Data[[var]]\n",
    "    model = SARIMAX(Final_Data.HBA.values, order=(1, 1, 0), seasonal_order=(0,1,0,12), exog=Final_Data[[var]].shift(1).values,enforce_invertibility=False)\n",
    "    model_fit = model.fit(disp=0)\n",
    "#   print(model_fit.summary())\n",
    "    starts=len(Final_Data.HBA.values)\n",
    "    ends=len(Final_Data.HBA.values)+1\n",
    "    output = model_fit.predict(start=starts, end=ends,\n",
    "    exog=new.loc[RFinal_Data.index[-i-(ends-starts-1)]:RFinal_Data.index[-i+1]].values, typ='levels')\n",
    "#     print(Final_Data[-14:])\n",
    "    o.append(output[ends-starts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating forecast and checking the MAPE of forecast"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "future_forecast = pd.DataFrame(future_forecast[:11],index = actual.index,columns=['Prediction'])\n",
    "actual_vs_prediction=future_forecast.join(actual.HBA,how='outer')\n",
    "\n",
    "future_forecast=np.exp(future_forecast.dropna())\n",
    "mape=(sum(np.abs(future_forecast.NEW_C-future_forecast.prediction_NEW_C)/future_forecast.NEW_C)/len(future_forecast.NEW_C))*100\n",
    "print(mape)\n",
    "\n",
    "\n",
    "# ploting the forcast \n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot_date(future_forecast.index,actual_vs_prediction,linestyle='-')\n",
    "plt.legend(['Predicted','Actual'],fontsize=20)\n",
    "plt.title('ARIMAX on HBA Monthly 12 months',fontsize=40)\n",
    "plt.tick_params('x',labelsize=20)\n",
    "plt.tick_params('y',labelsize=20)\n",
    "# plt.set_xlabel(Date_Column,fontsize=18)\n",
    "plt.savefig('Arema_API2_Monthly.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR Range of models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing endogenious variables based on MAPE\n",
    "\n",
    "\n",
    "\n",
    "def forecastvar(Final_Data,endos,exogs,month_ahead):\n",
    "    Final_Data=Final_Data.set_index(\"DATE\")\n",
    "    Final_Data=np.log(Final_Data.bfill().ffill())\n",
    "    model = VAR(endog=Final_Data[endos],exog=Final_Data[exogs].shift(month_ahead).bfill())\n",
    "    results = model.fit(maxlags=20, ic='aic')\n",
    "    lag_order = results.k_ar\n",
    "    a=results.forecast(Final_Data[endos].values[-lag_order:],month_ahead,exog_future=Final_Data[exogs].values[-month_ahead:])[month_ahead-1]\n",
    "    return(np.exp(a[0]))\n",
    "\n",
    "def forecastvartrend(Final_Data,endos,exogs,month_ahead):\n",
    "    Final_Data=Final_Data.set_index(\"DATE\")\n",
    "    Final_Data=np.log(Final_Data.bfill().ffill())\n",
    "    model = VAR(endog=Final_Data[endos],exog=Final_Data[exogs].shift(month_ahead).bfill())\n",
    "    ## Change in this line only\n",
    "    results = model.fit(maxlags=20, ic='aic',verbose=True, trend='ctt')\n",
    "    lag_order = results.k_ar\n",
    "    a=results.forecast(Final_Data[endos].values[-lag_order:],month_ahead,exog_future=Final_Data[exogs].values[-month_ahead:])[month_ahead-1]\n",
    "    return(np.exp(a[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of MAPE for variables "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "mape={}\n",
    "os.chdir('D:/bloomberg_Data/Undiffrenced/base_data')\n",
    "xls = pd.ExcelFile('FullData_201808.xlsx')\n",
    "STD=pd.read_excel(xls, 'API2_Monthly')\n",
    "\n",
    "\n",
    "STD=STD.set_index(\"DATE\")\n",
    "month_ahead=1\n",
    "\n",
    "exogs=\n",
    "indos=\n",
    "o={}\n",
    "RFinal_Data=np.log(STD.bfill().ffill())\n",
    "for i in range(19,0,-1):\n",
    "    Final_Data=RFinal_Data[:RFinal_Data.index[-i]]\n",
    "    model = VAR(endog=Final_Data[indos],exog=Final_Data[exogs].shift(month_ahead).bfill())\n",
    "    results = model.fit(maxlags=20, ic='aic')\n",
    "    lag_order = results.k_ar\n",
    "    o[Final_Data.tail(1).index[0]+relativedelta(months=month_ahead)]=results.forecast(Final_Data[indos].values[-lag_order:],month_ahead,exog_future=Final_Data[exogs].values[-month_ahead:])[month_ahead-1]\n",
    "\n",
    "new=pd.DataFrame(o).T\n",
    "new=new.rename(columns={0:'prediction_NEW_C',1:'Prediction_New_C'})\n",
    "future_forecast=new.join(RFinal_Data,how='left')[['prediction_NEW_C','NEW_C']]\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot_date(future_forecast.index,np.exp(future_forecast[['NEW_C','prediction_NEW_C']]),linestyle='-')\n",
    "plt.legend(['Actual', 'Predicted'],fontsize=20)\n",
    "plt.title('VAR on NEW_C Monthly',fontsize=40)\n",
    "plt.tick_params('x',labelsize=20)\n",
    "plt.tick_params('y',labelsize=20)\n",
    "# plt.savefig('C:/Users/HP/Desktop/tata_iq/3-month-Ahead-Forecast-NEW_C-Monthly/plot-results-Exo/'+j+'.jpg')\n",
    "plt.show()\n",
    "java=future_forecast\n",
    "future_forecast=np.exp(future_forecast.dropna())\n",
    "mape=(sum(np.abs(future_forecast.NEW_C-future_forecast.prediction_NEW_C)/future_forecast.NEW_C)/len(future_forecast.NEW_C))*100\n",
    "print(mape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# johansen test for cointegration of variables "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import johanson1\n",
    "johansen1.coint_johansen(Final_Data[['API2','']].values,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector error correction models if johanson test fails"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "model = VECM(endog=Final_Data[indos])\n",
    "results = model.fit()\n",
    "results.predict(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting time series autocorrelation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results.plot_acorr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Error Variance Decomposition (FEVD)\n",
    "\n",
    "Forecast errors of component j on k in an i-step ahead forecast can be decomposed using the orthogonalized impulse responses Θi:\n",
    "\n",
    "ωjk,i=∑i=0h−1(e′jΘiek)2/MSEj(h)MSEj(h)=∑i=0h−1e′jΦiΣuΦ′iej\n",
    "These are computed via the fevd function up through a total number of steps ahead:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://www.statsmodels.org/dev/vector_ar.html\n",
    "\n",
    "fevd = results.fevd(5)\n",
    "\n",
    "results.fevd(20).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impulse responce function for equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L=results.irf()\n",
    "L.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger causality\n",
    "\n",
    "results.test_causality('realgdp', ['realinv', 'realcons'], kind='f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality\n",
    "Whiteness of residuals\n",
    "Dynamic Vector Autoregressions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var = DynamicVAR(data, lag_order=2, window_type='expanding')\n",
    "var.forecast(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
